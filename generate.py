import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import psycopg2

# Load a smaller model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Connect to the PostgreSQL database
conn = psycopg2.connect(
    dbname="cve",
    user="postgres",
    password="admin",
    host="csye7125-consumer-headless.ns3.svc.cluster.local",
    port="5432"
)

# Create a cursor object
cur = conn.cursor()

# Query to select the JSON data from the specified column
cur.execute("SELECT data FROM cve.cve")

# Fetch all rows from the executed query
rows = cur.fetchall()

# Close the cursor and connection
cur.close()
conn.close()

# Extract the data
data = [row[0] for row in rows]

# Function to concatenate all relevant fields
def concatenate_record(record):
    fields = []

    # Ensure 'containers' and 'cna' exist and are not None
    cna = record.get('containers', {}).get('cna', {})

    # Add product, vendor, version, descriptions, etc.
    affected = cna.get('affected', [])
    if affected:  
        for item in affected:
            fields.append(item.get('vendor', ''))
            fields.append(item.get('product', ''))
            versions = item.get('versions', [])
            if versions: 
                fields.extend([v.get('version', '') for v in versions])
    
    descriptions = cna.get('descriptions', [])
    if descriptions:  
        for desc in descriptions:
            fields.append(desc.get('value', ''))
    
    problem_types = cna.get('problemTypes', [])
    if problem_types:  
        for problem in problem_types:
            fields.append(problem['descriptions'][0].get('description', ''))

    if 'cveMetadata' in record:
        fields.append(record['cveMetadata'].get('cveId', ''))
        fields.append(record['cveMetadata'].get('state', ''))
        fields.append(record['cveMetadata'].get('datePublished', ''))
        fields.append(record['cveMetadata'].get('assignerShortName', ''))
    
    # Combine all fields into one string
    combined_text = ' '.join(fields)
    return combined_text

# Prepare the data for indexing
combined_texts = [concatenate_record(record) for record in data]

# Generate embeddings for the entire dataset
embeddings = model.encode(combined_texts, convert_to_tensor=False)
embeddings = np.array(embeddings)

# Normalizing the embeddings to calculate cosine similarity
embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

# Set the dimensionality and create the HNSW index
dimension = model.get_sentence_embedding_dimension()
index = faiss.IndexHNSWFlat(dimension, 32)  # 32 is the number of neighbors

# Add embeddings to the index
index.add(embeddings)

# Save the FAISS index to disk
faiss.write_index(index, '/mnt/data/faiss_index.index')
